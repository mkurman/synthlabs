
import { extractJsonFields } from './utils/jsonFieldExtractor.ts';

const failingInput = `<think>We need to provide answer: best practices for streaming large response in Java servlet. Provide reasoning trace and final answer. Must avoid common pitfalls: set headers, use buffer size, close streams, avoid temp file, consider async support (Servlet 3.0 async), use HttpServletResponse's getOutputStream, maybe use Transfer-Encoding: chunked, set content length if known, use caching headers, use GZIP if needed. Also mention using BufferedOutputStream, avoid reading entire file into memory, use try-with-resources. Also mention using AWS SDK's TransferManager or S3ObjectInputStream. Provide code example. Also mention not to use local temp file, unless you need random access; but streaming is fine. Provide best practices: use async, non-blocking I/O, use smaller buffer, handle exceptions, set content type, add ETag, etc. Provide reasoning trace with symbols. Let's craft.</think>{
  "reasoning": "***\\n### 1. Query Decomposition\\n\\nThe user asks for\\n- a Java servlet that streams large (>200 k) responses,\\n- best‑practice guidance,\\n- whether a temporary file + extra thread is needed.\\n\\nWe must:\\n◐ Show that direct streaming from the data source (DB or S3) to the response OutputStream avoids OOM,\\n◐ Explain servlet‑async / non‑blocking I/O as optional performance improvement,\\n◐ List common pitfalls (buffer size, missing headers, keeping data in memory),\\n◐ Provide a clean, production‑ready code snippet.\\n\\n### 2. Knowledge Assessment\\n\\n● The servlet API provides \`HttpServletResponse#getOutputStream()\` which returns a binary stream.\\n● AWS S3 SDK can give an \`InputStream\` directly via \`S3Object#getObjectContent()\`.\\n● Java 7+ offers try‑with‑resources to automatically close streams.\\n● Servlet 3.0+ supports async mode (\`request.startAsync()\`) to free the request thread while I/O is in progress.\\n● HTTP supports *chunked* transfer when \`Content‑Length\` is unknown; the servlet container handles this automatically.\\n● Common mistakes: reading the whole file into a \`byte[]\`, using a tiny buffer (1 k) which forces many system calls, forgetting \`resp.flushBuffer()\`, or not setting \`Content‑Type\`.\\n\\n### 3. Logical Reasoning\\n\\n1. **Do not materialise the whole payload in memory** – read a small block, write it, repeat.\\n   - This keeps memory usage bounded by the buffer size, not the file size.\\n2. **Use a reasonable buffer** – 8 k or 16 k is a good compromise between system‑call overhead and memory usage.\\n3. **Set the correct MIME type** – \`resp.setContentType(...)\` or \`resp.setContentType(\\"application/octet-stream\\")\` for unknown types.\\n4. **Optionally set \`Content‑Disposition\`** to suggest a filename.\\n5. **If the file size is known** (e.g., S3 object metadata \`ContentLength\`) set \`resp.setContentLengthLong(length)\`; otherwise rely on chunked transfer.\\n6. **Leverage async I/O** – \`request.startAsync()\` returns an \`AsyncContext\`; you can perform the stream in a separate thread while the container keeps the connection open.\\n   - For simple streaming the container’s I/O thread is usually sufficient, but async is a safety net if you anticipate long‑running I/O.\\n7. **Avoid intermediate temp files** – they introduce extra disk I/O and potential cleanup issues; the stream‑to‑stream pattern is sufficient for most use‑cases.\\n8. **Handle exceptions gracefully** – log the error and send an appropriate HTTP error code (e.g., 500) without leaking sensitive info.\\n\\n### 4. Conclusion\\n\\nThe optimal pattern is:\\n- Enter servlet (sync or async),\\n- Acquire an \`InputStream\` from the data source (DB, S3, etc.),\\n- Write in a loop using a moderate buffer, \\n- Close streams via try‑with‑resources, \\n- Let the container manage chunked transfer.\\n\\nThis yields bounded memory usage, good throughput, and minimal extra I/O.\\n\\n### 5. Code Example\\n\\n\`\`\`java\\n@WebServlet(name = \\"S3ImageServlet\\", urlPatterns = {\\"/image/*\\"})\npublic class S3ImageServlet extends HttpServlet {\\n\\n    private final S3Client s3Client = new S3Client(); // your configured client\\n    private static final int BUFFER_SIZE = 16 * 1024; // 16 k buffer\\n\\n    @Override\\n    protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException {\\n        // 1. Parse the requested key from the path\\n        String key = req.getPathInfo(); // e.g., \\"/myFolder/picture.jpg\\"\\n        if (key == null || key.isEmpty()) {\\n            resp.sendError(HttpServletResponse.SC_BAD_REQUEST, \\"Missing key\\");\\n            return;\\n        }\\n\\n        // 2. Optional: start async processing (free the servlet thread)\\n        AsyncContext async = req.startAsync();\\n        async.setTimeout(0); // let it run indefinitely (or set a realistic timeout)\\n\\n        async.start(() -> {\\n            try {\\n                // 3. Retrieve object from S3\\n                S3Object s3obj = s3Client.getObject(new GetObjectRequest(\\"my-bucket\\", key));\\n\\n                // 4. Set response headers before any data is written\\n                resp.setContentType(getServletContext().getMimeType(key));\\n                resp.setHeader(\\"Cache-Control\\", \\"public, max-age=31536000\\"); // optional caching\\n                // If size is known we can set it; otherwise container will use chunked transfer\\n                resp.setContentLengthLong(s3obj.getObjectMetadata().getContentLength());\\n\\n                // 5. Stream data\\n                try (ServletOutputStream out = resp.getOutputStream();\\n                     S3ObjectInputStream in = s3obj.getObjectContent()) {\\n                    byte[] buf = new byte[BUFFER_SIZE];\\n                    int read;\\n                    while ((read = in.read(buf)) > 0) {\\n                        out.write(buf, 0, read);\\n                    }\\n                }\\n                resp.flushBuffer();\\n            } catch (Exception e) {\\n                // Log and send a generic error\\n                log(\\"Error streaming S3 object\\", e);\\n                try { resp.sendError(HttpServletResponse.SC_INTERNAL_SERVER_ERROR); } catch (IOException ignore) {}\\n            } finally {\\n                async.complete();\\n            }\\n        });\\n    }\\n}\\n\`\`\`\\n\\nKey points highlighted in the code:\\n- \`BUFFER_SIZE\` is a trade‑off; 16 k reduces system calls compared to 1 k.\\n- \`try‑with‑resources\` guarantees streams are closed even on exception.\\n- Async context is optional; it frees the request thread while I/O continues.\\n- No temporary file is created – data flows directly from S3 to the client.\\n\\n### 6. Final Takeaway\\n\\n> **Stream from source → servlet output, never materialise the whole payload.** Use a moderate buffer, set MIME/length headers, optionally run the I/O in async mode, and let the container handle chunked transfer. This pattern satisfies the memory‑constraint and scales with request volume.\\n\\n",
  "answer": "Yes – the best practice is to stream directly from the data source (database, S3, etc.) to the \`HttpServletResponse\`’s \`OutputStream\` using a moderate‑sized buffer. Avoid loading the whole file into a \`byte[]\` or writing to a temporary disk file; both waste memory or add unnecessary I/O. Use a try‑with‑resources block, set the correct MIME type and, if known, the \`Content‑Length\`. Optionally start async processing (\`request.startAsync()\`) so the servlet thread is freed while the I/O thread copies the data. A typical implementation looks like the code above, which reads 16 k blocks from the S3 input stream and writes them to the response output stream, guaranteeing bounded memory usage and good throughput."
}`;

console.log("Input length:", failingInput.length);
console.log("Starts with think:", failingInput.startsWith("<think>"));

const extracted = extractJsonFields(failingInput);
console.log("Extracted:", JSON.stringify(extracted, null, 2));

if (!extracted.answer && !extracted.reasoning) {
    console.log("FAIL: No fields extracted");
} else {
    console.log("SUCCESS: Fields extracted");
}
